<!DOCTYPE html>
<html lang="fa">
 <meta content="text/html;charset=utf-8" http-equiv="content-type"/>
 <head>
  <title>
   Systematic review and meta-analysis
  </title>
  <link href="d/css/1.css" rel="stylesheet"/>
 </head>
 <body>
  <!-- Start Main Menu Area -->
  <br/>
  <br/>
  <br/>
  <br/>
  <div class="container">
   <div class="row">
   </div>
  </div>
  <section class="teacher-area ptb-10">
   <div class="container">
   </div>
   <section class="course-details-area ptb-60">
    <div class="container">
     <div class="uptodate2" style="padding:50px 0px 50px 0px;direction: ltr;text-align: left;">
      <h1 style="text-align:center">
       Systematic review and meta-analysis
      </h1>
      <div class="utdArticleSection utdStyle" id="topicContent">
       <div id="topicTitle">
        Systematic review and meta-analysis
       </div>
       <div class="authorSectionElem">
        <div class="authorsElementsLeft">
         <dl id="topicContributors">
          <dt>
           <span>
           </span>
           Authors:
          </dt>
          <dd>
           <a class="contributor contributor_credentials" rel="noopener noreferrer" target="_blank">
            Ethan Balk, MD, MPH
           </a>
          </dd>
          <dd>
           <a class="contributor contributor_credentials" rel="noopener noreferrer" target="_blank">
            Peter A L Bonis, MD
           </a>
          </dd>
          <dt>
           <span>
           </span>
           Section Editor:
          </dt>
          <dd>
           <a class="contributor contributor_credentials" rel="noopener noreferrer" target="_blank">
            Joann G Elmore, MD, MPH
           </a>
          </dd>
          <dt>
           <span>
           </span>
           Deputy Editor:
          </dt>
          <dd>
           <a class="contributor contributor_credentials" rel="noopener noreferrer" target="_blank">
            Carrie Armsby, MD, MPH
           </a>
          </dd>
         </dl>
        </div>
        <div class="authorsElementsRight">
         <div id="literatureReviewDate">
          <div class="litReviewSingleLine">
           <bdi>
            <span class="emphasis">
             Literature review current through:
            </span>
            Jan 2024.
           </bdi>
          </div>
          <div class="litReviewSingleLine">
           <bdi>
            <span class="emphasis">
             This topic last updated:
            </span>
            Aug 31, 2023.
           </bdi>
          </div>
         </div>
        </div>
       </div>
       <div id="topicWhatsNewContainer">
       </div>
       <div id="topicText">
        <p class="headingAnchor" id="H2326445">
         <span class="h1">
          INTRODUCTION
         </span>
         <span class="headingEndMark">
          —
         </span>
         This topic review will provide an overview of how systematic reviews and meta-analyses are conducted and how to interpret them. In addition, it will provide a summary of methodologic terms commonly encountered in systematic reviews and meta-analyses.
        </p>
        <p>
         A broader discussion of evidence-based medicine and a glossary of biostatistical and epidemiological terms are presented separately. (See
         <a class="medical medical_review" href="/z/d/html/2763.html" rel="external">
          "Evidence-based medicine"
         </a>
         and
         <a class="medical medical_review" href="/z/d/html/2759.html" rel="external">
          "Glossary of common biostatistical and epidemiological terms"
         </a>
         .)
        </p>
        <p class="headingAnchor" id="H2327453">
         <span class="h1">
          KEY DEFINITIONS
         </span>
         <span class="headingEndMark">
          —
         </span>
         The terms systematic review and meta-analysis are often used together, but they are not interchangeable. Not all systematic reviews include meta-analyses, though many do.
        </p>
        <p>
         These terms are defined here since they are used throughout this topic. A glossary of other relevant terms is provided at the end of this topic. (See
         <a class="local">
          'Glossary of terms'
         </a>
         below.)
        </p>
        <p class="headingAnchor" id="H78553622">
         <span class="h2">
          Systematic review
         </span>
         <span class="headingEndMark">
          —
         </span>
         A systematic review is a comprehensive summary of all available evidence that meets predefined eligibility criteria to address a specific clinical question or range of questions. It is based upon a rigorous process that incorporates [
         <a href="#rid1">
          1,2
         </a>
         ]:
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Determination of research question, including study eligibility and methodology
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Systematic identification of studies that have evaluated the specific research question(s)
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Critical appraisal of the studies
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Meta-analyses (not always performed) (see
         <a class="local">
          'Meta-analysis'
         </a>
         below)
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Presentation of key findings (eg, in a summary of findings table)
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Explicit assessment of the limitations of the evidence (ie, rating the certainty [or quality] of the body of evidence)
        </p>
        <p>
        </p>
        <p>
         Systematic reviews contrast with "narrative" reviews and textbook chapters which generally do not exhaustively review the literature. In addition, narrative reviews lack transparency in the selection and interpretation of supporting evidence, generally do not provide a quantitative synthesis of the data, and may be biased if the included evidence was selected to support a preconceived conclusion rather than deriving conclusions from the entire body of evidence.
        </p>
        <p class="headingAnchor" id="H78553629">
         <span class="h2">
          Meta-analysis
         </span>
         <span class="headingEndMark">
          —
         </span>
         Meta-analysis, which is commonly included in systematic reviews, is the statistical method of quantitatively combining or pooling results from different studies. It can be used to provide overall pooled effect estimates. For example, if a drug was evaluated in multiple placebo-controlled trials that all reported the same outcome, meta-analysis can be used to estimate a pooled relative risk for the drug's overall effect based upon all of the trials. Meta-analysis can also be used to pool other types of data such as studies on diagnostic accuracy (eg, pooled estimates on sensitivity and specificity) and epidemiologic studies (eg, pooled incidence or prevalence rates; pooled odds ratio for strength of association). Meta-regression and network meta-analysis (NMA) are enhancements to traditional meta-analysis. (See
         <a class="local">
          'Meta-regression'
         </a>
         below and
         <a class="local">
          'Network meta-analysis'
         </a>
         below.)
        </p>
        <p class="headingAnchor" id="H3448483323">
         <span class="h1">
          ADVANTAGES OF SYSTEMATIC REVIEW AND META-ANALYSIS
         </span>
         <span class="headingEndMark">
          —
         </span>
         Clinical decisions in medicine ideally should be based upon guidance from a comprehensive assessment of the body of available knowledge. A single clinical trial, even a large one, is seldom sufficient to provide a confident answer to a clinical question. Indeed, one analysis suggested that most research claims are ultimately proven to be incorrect or inaccurate when additional studies have been performed [
         <a href="#rid3">
          3
         </a>
         ]. At the same time, it is well established that large randomized controlled trials do not always confirm the results of prior meta-analyses [
         <a href="#rid4">
          4-6
         </a>
         ]. The "truth" needs to be understood by examining all sources of data as critically and objectively as possible.
        </p>
        <p>
         There are several potential benefits to performing systematic analysis, which may also include meta-analysis:
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Unique aspects to a single randomized trial, involving the participating patient population, protocol, setting in which the trial is performed, or expertise of the involved clinicians, may limit its generalizable to other settings or individual patients. The conclusions of systematic reviews are likely to be more generalizable than single studies.
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Combining studies in meta-analyses increases the sample size and generally produces more precise estimates of the effect size (ie, estimates that have smaller confidence intervals) than a single randomized trial. Meta-analysis may also allow exploration of reasons for heterogeneity across studies to allow conclusions beyond what can be gleaned from individual studies. (See
         <a class="local">
          'Exploration of heterogeneity'
         </a>
         below.)
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Clinicians rarely have the time or resources to critically evaluate the body of evidence relevant to a particular clinical question, and a systematic review can facilitate this investigation. (See
         <a class="medical medical_review" href="/z/d/html/2763.html" rel="external">
          "Evidence-based medicine", section on 'Categories of evidence'
         </a>
         .)
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         In contrast with narrative review articles, most systematic reviews focus on narrow, clearly defined questions and include all eligible studies, not just studies chosen by the author. Systematic reviews are therefore less prone to bias since they derive conclusions from the entire body of evidence, whereas narrative reviews may be biased if the included evidence was selected to support a preconceived conclusion.
        </p>
        <p>
        </p>
        <p>
         Systematic review and meta-analysis are methods to synthesize the available evidence using an explicit, transparent approach that considers the strengths and weaknesses of the individual studies, populations and interventions, and specific outcomes that were assessed. Individual practitioners, policymakers, and guideline developers can use well-conducted systematic reviews to determine best patient management decisions. Organizations that develop guidelines can use the results of systematic reviews and meta-analyses to provide evidence-based recommendations for care.
        </p>
        <p class="headingAnchor" id="H2326452">
         <span class="h1">
          STEPS TO CONDUCTING A SYSTEMATIC REVIEW AND META-ANALYSIS
         </span>
        </p>
        <p class="headingAnchor" id="H4054246890">
         <span class="h2">
          Overview
         </span>
         <span class="headingEndMark">
          —
         </span>
         Several steps are essential for conducting a systematic review or meta-analysis. These include:
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Formulating research questions (see
         <a class="local">
          'Formulating research questions'
         </a>
         below)
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Developing a protocol (see
         <a class="local">
          'Developing a protocol'
         </a>
         below)
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Searching for the evidence (see
         <a class="local">
          'The literature search'
         </a>
         below)
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Assessing the quality of studies (often referred to as the risk of bias [RoB] assessment)
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Summarizing and displaying results (eg, using forest pots and a summary of findings table, as shown in the figure (
         <a class="graphic graphic_figure graphicRef121523" href="/z/d/graphic/121523.html" rel="external">
          figure 1
         </a>
         )) (see
         <a class="local">
          'Forest plot'
         </a>
         below)
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Exploring reasons for heterogeneity across studies (see
         <a class="local">
          'Exploration of heterogeneity'
         </a>
         below)
        </p>
        <p>
        </p>
        <p>
         The basic steps, along with limitations that should be considered, are discussed here. While this topic review focuses on meta-analysis of randomized controlled trials, many of the methods and issues apply equally to meta-analyses of other comparative studies, noncomparative (single group) and other observational studies, and studies of diagnostic tests. An overview of approaches to systematic review and meta-analysis is provided in a table (
         <a class="graphic graphic_table graphicRef80657" href="/z/d/graphic/80657.html" rel="external">
          table 1
         </a>
         ).
        </p>
        <p>
         The updated 2020 Preferred Reporting Items of Systematic reviews and Meta-Analyses (PRISMA)
         <a class="external" href="/external-redirect?target_url=http%3A%2F%2Fwww.prisma-statement.org%2FPRISMAStatement%2FPRISMAStatement.aspx&amp;token=mGCp3jf%2BU%2BRF8NnZx3snHv23JvyZHFuw2BQvvMqH%2FTjTMtv75T4gxC%2BcKZMfaCgExPonCgH29dtCpZpCzjPV8g%3D%3D&amp;TOPIC_ID=16293" target="_blank">
          statement
         </a>
         emphasizes that systematic reviews should provide the protocol, data, and assessments of RoB from individual studies with sufficient transparency to allow the reader to verify the results [
         <a href="#rid1">
          1
         </a>
         ]. It underscores the basic questions that the clinician and investigator should ask when interpreting a systematic review. The PRISMA
         <a class="external" href="/external-redirect?target_url=http%3A%2F%2Fwww.prisma-statement.org%2FPRISMAStatement%2FChecklist&amp;token=mGCp3jf%2BU%2BRF8NnZx3snHv23JvyZHFuw2BQvvMqH%2FTjmmF7gwUdiFGoE%2Bmu%2Fql6TihM0fAGB%2Brv8BF%2By5NpFsQ%3D%3D&amp;TOPIC_ID=16293" target="_blank">
          website
         </a>
         provides checklists for the items that should be included in a systematic review. Several "
         <a class="external" href="/external-redirect?target_url=http%3A%2F%2Fwww.prisma-statement.org%2FExtensions%2FDefault.aspx&amp;token=i20xYI8aSBFiAGc5wtYRs651Bna3iA4Qk4trdocUrr%2B98tPGhICW5bd%2Bx4knFhqLMwhc4JBEU3LrKG2dfTXb4g%3D%3D&amp;TOPIC_ID=16293" target="_blank">
          extensions
         </a>
         " to PRISMA have been developed for specific types of systematic reviews or meta-analyses (eg, PRISMA for diagnostic test accuracy, PRISMA for individual patient data analyses, PRSMA for network meta-analysis) [
         <a href="#rid7">
          7
         </a>
         ]. In addition, readers of systematic reviews should assess the relevance to their own practice with regards to the populations, settings, interventions, and outcomes studied. (See
         <a class="local">
          'Reading and interpreting a systematic review'
         </a>
         below.)
        </p>
        <p>
         In 2011, the Institute of Medicine published recommended standards for developing systematic reviews, which remain pertinent [
         <a href="#rid8">
          8
         </a>
         ]. While these standards principally apply to publicly funded systematic reviews of comparative effectiveness research that focus specifically on treatments, most of the standards pertain to all systematic reviews. The United States Agency for Healthcare Research and Quality also has an ongoing series of articles that form a
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Feffectivehealthcare.ahrq.gov%2Fproducts%2Fcollections%2Fcer-methods-guide&amp;token=gVeQyirfVpRJ1R1tVRZj9YxjMQE4yIzw13hkaUVWgC02yoROeHFmPJ6As0q12Jl3WvkxwNa8H1HKN6%2FnbE9PSgfa1OGFEBRhadfI7iVG%2F1I%3D&amp;TOPIC_ID=16293" target="_blank">
          Methods Guide for Comparative Effectiveness Reviews
         </a>
         for its Evidence-based Practice Center program and related reviews. This guide principally applies to large overarching systematic reviews but provides insights and recommendations for addressing a broad range of topics, studies, and methodological approaches.
        </p>
        <p>
         The Cochrane Collaboration (an international organization that advances systematic reviews and meta-analyses) also provides guidance for conducting systematic reviews and meta-analyses specific to the effects of healthcare interventions [
         <a href="#rid9">
          9
         </a>
         ].
        </p>
        <p class="headingAnchor" id="H831494461">
         <span class="h2">
          Formulating research questions
         </span>
         <span class="headingEndMark">
          —
         </span>
         Research questions in systematic reviews are analogous to the research hypotheses of primary research studies. They should be focused and defined clearly since they determine the scope of research the systematic review will address [
         <a href="#rid10">
          10
         </a>
         ].
        </p>
        <p>
         Broad questions that cover a range of topics may not be directly answerable and are not appropriate for systematic reviews or meta-analyses. As an example, the question "What is the best treatment for chronic hepatitis B?" would need to be broken down into several smaller well-focused questions that could be addressed in individual and complementary systematic reviews. Examples of appropriate key questions may include, "How does
         <a class="drug drug_general" data-topicid="8816" href="/z/d/drug information/8816.html" rel="external">
          entecavir
         </a>
         compare with placebo for achieving hepatitis B e antigen (HBeAg) seroconversion in patients with chronic HBeAg-positive hepatitis B?" and "What is the relationship between hepatitis B genotypes and response rates to entecavir?" These and other related questions would be addressed individually and then, ideally, considered together to answer the more general question.
        </p>
        <p>
         Research questions for studies of the effectiveness of interventions are commonly formulated according to the "PICO" method, which fully defines the
         <strong>
          P
         </strong>
         opulation,
         <strong>
          I
         </strong>
         ntervention,
         <strong>
          C
         </strong>
         omparator, and
         <strong>
          O
         </strong>
         utcomes of interest [
         <a href="#rid10">
          10
         </a>
         ]. The acronym "PICOD" is sometimes used to indicate that investigators must also specify which study designs are appropriate to include (eg, all comparative studies versus only randomized trials). Other eligibility criteria may include the timing or setting of care. Variations of these criteria should be used for systematic reviews of other study designs, such as of cohort studies (without a comparator), studies of exposures (instead of interventions), studies of diagnostic tests, and qualitative research.
        </p>
        <p class="headingAnchor" id="H2326459">
         <span class="h2">
          Developing a protocol
         </span>
         <span class="headingEndMark">
          —
         </span>
         A written protocol serves to minimize bias and to ensure that the review is implemented according to reproducible steps. A systematic review should describe its research question (and its component PICOD elements) and the review methodology, including the search strategy and approaches to analyzing and summarizing the data. Ideally, the protocol should be a collaborative effort that includes both clinical and methodology experts.
        </p>
        <p>
         Publication of protocols can be useful to prevent unnecessary duplication of efforts and to enhance transparency of the systematic review. A voluntary registry,
         <a class="external" href="/external-redirect?target_url=http%3A%2F%2Fwww.crd.york.ac.uk%2FPROSPERO%2F&amp;token=8WmZyxtCPDhnIAxHBR5GfGvJ6Q1klmyn3WHXY07Oqvwj31GdzXJLenhYYw7R1wi6&amp;TOPIC_ID=16293" target="_blank">
          PROSPERO
         </a>
         , was established in 2011. The database contains protocol details for systematic reviews that have health-related outcomes.
        </p>
        <p class="headingAnchor" id="H2326473">
         <span class="h2">
          The literature search
         </span>
        </p>
        <p class="headingAnchor" id="H3713897726">
         <span class="h3">
          Performing the search
         </span>
         <span class="headingEndMark">
          —
         </span>
         The literature search should be systematic and comprehensive to minimize error and bias. Most systematic reviews start with searches in at least two electronic databases of the literature. Medline is almost universally used (eg, through the
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fpubmed.ncbi.nlm.nih.gov%2F&amp;token=gvw6LY0R2TrQ95gqn9D8GvuqYvz7FdGy2HDGCRWH4q0SB51MKPd8S63AKJ2Gdtkt&amp;TOPIC_ID=16293" target="_blank">
          PubMed
         </a>
         interface); other commonly searched databases include
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fwww.embase.com%2Flanding%3Fstatus%3Dgrey&amp;token=NyRkA1K%2BEfcjom0B%2BqrukmhcdoilgLJKnCBbJQESLRpwm%2Fpb0x6AZXdXsrSon9LWmclJOKPeeo5KIjipbjdRUw%3D%3D&amp;TOPIC_ID=16293" target="_blank">
          Embase
         </a>
         and the . Inclusion of additional databases should be considered for specialized topics such as mental health, complementary or alternative medicine, quality of care, and nursing. Electronic searches should be supplemented by searches of the bibliographies of retrieved articles and relevant review articles and by studies known to domain experts.
        </p>
        <p>
         Some researchers attempt to incorporate unpublished data (so called "grey literature") to diminish the risks of publication bias (selective publication of studies, possibly based on their results), reporting bias (selective reporting of study results, possibly based on statistical significance), and to include data that are evolving rapidly and not yet published [
         <a href="#rid11">
          11
         </a>
         ]. The importance of including unpublished data sources in systematic reviews and meta-analysis is uncertain [
         <a href="#rid12">
          12
         </a>
         ]. There is no standard definition of grey literature, but it generally refers to information obtained from sources other than published, peer-reviewed articles. This may include conference abstracts and proceedings, clinical trial registries (eg,
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fclinicaltrials.gov%2F&amp;token=nbsXe3i4qfUea5RCPRHtOc5ko1zksgMm05jF47%2FUSB3l0tvt%2BGAm3dUe1JoUg9nw&amp;TOPIC_ID=16293" target="_blank">
          ClincalTrials.gov
         </a>
         registry), adverse events databases, government agency databases and documents (eg, US Food and Drug Administration), unpublished industry data, dissertations, and online sites.
        </p>
        <p class="headingAnchor" id="H2326480">
         <span class="h3">
          Publication and reporting bias
         </span>
         <span class="headingEndMark">
          —
         </span>
         Reporting bias refers to bias that results from incomplete publishing or reporting of available research. This is a common concern and a potentially important limitation of systematic review since the missing data may affect the validity of systematic reviews [
         <a href="#rid13">
          13
         </a>
         ]. There are two main categories of reporting bias:
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Publication bias – Compared with positive studies, negative studies may take longer to be published or may not be published at all [
         <a href="#rid14">
          14
         </a>
         ]. This is referred to as "publication bias."
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Outcome reporting bias – "Outcome reporting bias" refers to the concern that a study may only include outcomes that are favorable and significant in the published report, while nonsignificant or unfavorable outcomes are selectively not reported. This may occur due to active suppression of "negative" findings or merely because of space limitations of the publication.
        </p>
        <p>
        </p>
        <p>
         Several methods have been developed to evaluate whether publication bias is present. However, they all involve major assumptions about possible missing studies [
         <a href="#rid15">
          15
         </a>
         ]. Any evaluation of publication bias should not be considered definitive, but rather only exploratory in nature.
        </p>
        <p>
         A commonly used method for assessing publication bias is the funnel plot, which is a scatter plot displaying the relationship between the weight of the study (eg, study size or standard error) and the observed effect size (
         <a class="graphic graphic_figure graphicRef65012" href="/z/d/graphic/65012.html" rel="external">
          figure 2
         </a>
         ) [
         <a href="#rid16">
          16
         </a>
         ]. An asymmetric appearance, especially due to the absence of smaller negative studies, can suggest unpublished data. However, this assessment is not definitive since asymmetry could be due to factors other than unpublished negative studies (such as population heterogeneity or study quality) [
         <a href="#rid13">
          13,17-19
         </a>
         ]. Funnel plot assessments are generally considered unreliable when there are &lt;10 studies included in a meta-analysis [
         <a href="#rid20">
          20
         </a>
         ].
        </p>
        <p>
         Other methods to evaluate reporting bias include the "trim and fill" method, "modeling selection process," and testing for an excess of significant findings [
         <a href="#rid21">
          21-24
         </a>
         ]. These methods are beyond the scope of this topic.
        </p>
        <p class="headingAnchor" id="H2326487">
         <span class="h2">
          Risk of bias assessment
         </span>
         <span class="headingEndMark">
          —
         </span>
         The quality of an individual study has been defined as the "confidence that the trial design, conduct, and analysis has minimized or avoided biases" [
         <a href="#rid25">
          25
         </a>
         ]. The risk of bias (RoB) assessment (sometimes referred to as "quality assessment") represents the extent to which trial design and methodology prevented systematic error and can help explain differences in the results of systematic reviews.
        </p>
        <p>
         The primary value of the RoB assessment of individual studies in the meta-analysis is to determine the degree of confidence that the pooled effect estimate reflects the "truth" as best as it can be measured. One would be more likely to have high confidence in conclusions based upon "high-quality" (ie, low RoB) studies rather than "low-quality" (ie, high RoB) studies. Differences in RoB of individual studies can also be explored to help explain heterogeneity (eg, does the effect in low RoB studies differ from that in high RoB?).
        </p>
        <p>
         The process of assessing study quality is not straightforward. Numerous RoB assessment systems are available. Different study designs have different methodological concerns and thus different RoB assessment tools are used depending on the methodology of the individual studies. Commonly used tools (among many others) include:
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         For RCTs:
        </p>
        <p>
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         Original Cochrane RoB tool for randomized controlled trials (with 7 questions [
         <a href="#rid26">
          26
         </a>
         ])
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         More complex revision of this tool,
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fsites.google.com%2Fsite%2Friskofbiastool%2Fwelcome%2Frob-2-0-tool%2Fcurrent-version-of-rob-2&amp;token=aKCfSuifXSDG%2F9XK9XSQhqw0XfW4IT%2BN1zz3RkNMK8YsLlUstqMtlMp%2BgA%2BKprtx%2BcDIDZfbz7xQ2kBBxmjD9rkubgrkD3cp5E44Deqo3Y8%3D&amp;TOPIC_ID=16293" target="_blank">
          RoB 2
         </a>
         (with 5 overarching questions and 22 subquestions [
         <a href="#rid27">
          27
         </a>
         ])
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         CASP (Critical Appraisal Skills Programme) Randomised Controlled Trial Checklist (with
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fcasp-uk.net%2Fcasp-tools-checklists%2F&amp;token=mc8yydvJzVuwRYckNZjKlTxuGIee4Lr7OG7utc%2BjCfl0mm1G4XXg1UUqGylHHNLo&amp;TOPIC_ID=16293" target="_blank">
          11 questions
         </a>
         )
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         NHLBI (National Heart, Lung, and Blood Institute) Quality Assessment of Controlled Intervention Studies (with
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fwww.nhlbi.nih.gov%2Fhealth-topics%2Fstudy-quality-assessment-tools&amp;token=l4kOfBMiE98gtdyzjrpRMsP5xe4A2vYicw%2BObzH34sPShecgcXF4ZZJD%2Fo0qBTZCiRc1h2F8SA1tTns2R3uyYHBsnS6bFjsZGkUtCNTmIhM%3D&amp;TOPIC_ID=16293" target="_blank">
          14 questions
         </a>
         )
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         For observational comparative studies (of various designs):
        </p>
        <p>
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         The
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fsites.google.com%2Fsite%2Friskofbiastool%2Fwelcome%2Fhome&amp;token=fS79j%2FZG7ECJ4dswsfa8P%2BbcP3njMARgVAqKo8t7eSig%2FSp6rOd%2FLsKViVpqOLBgTzskabmxMeC%2BB4iCRNcRHA%3D%3D&amp;TOPIC_ID=16293" target="_blank">
          ROBINS-I tool
         </a>
         (Risk Of Bias In Non-randomized Studies of Interventions, with 7 overarching questions and 31 subquestions [
         <a href="#rid28">
          28
         </a>
         ])
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         CASP Cohort Study Checklist (with
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fcasp-uk.net%2Fcasp-tools-checklists%2F&amp;token=mc8yydvJzVuwRYckNZjKlTxuGIee4Lr7OG7utc%2BjCfl0mm1G4XXg1UUqGylHHNLo&amp;TOPIC_ID=16293" target="_blank">
          12 questions
         </a>
         )
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         Joanna Briggs Institute (JBI) Checklist for Cohort Studies (with
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fjbi.global%2Fcritical-appraisal-tools&amp;token=Pxy8Vgz5a3zcFqUCJoMd6lGrMkmibez7Lck%2Bpaek%2BEByCSEDw%2BUjLj9HQ61Iof6SkUoPthV8sRSCT1BckXVEZA%3D%3D&amp;TOPIC_ID=16293" target="_blank">
          11 questions
         </a>
         )
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         For studies on diagnostic tests:
        </p>
        <p>
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         JBI Checklist for Diagnostic Test Accuracy Studies (with
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fjbi.global%2Fcritical-appraisal-tools&amp;token=Pxy8Vgz5a3zcFqUCJoMd6lGrMkmibez7Lck%2Bpaek%2BEByCSEDw%2BUjLj9HQ61Iof6SkUoPthV8sRSCT1BckXVEZA%3D%3D&amp;TOPIC_ID=16293" target="_blank">
          10 questions
         </a>
         ) [
         <a href="#rid29">
          29
         </a>
         ]
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         Different methodologists use different tools depending on available time and resources, needs and purpose of the given review, and philosophical differences among researchers about the relative importance of different "quality" factors. Importantly, the assessment of a study's RoB can be limited by the need to rely on information presented in the manuscript [
         <a href="#rid30">
          30
         </a>
         ].
        </p>
        <p>
        </p>
        <p>
         For randomized trials, the RoB assessment typically considers the following factors:
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Randomization method
         </strong>
         – Some "randomization" methods are not truly random, which can be a source of bias. For example, a computer algorithm is generally preferred over a system based on day of the week or other nonrandom method.
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Allocation concealment
         </strong>
         – Allocation is the assignment of study participants to a treatment group. It occurs between randomization and implementation of the intervention. Allocation should be adequately concealed from the study personnel. A study may be biased if allocation is not concealed. For example, if the study used unsealed envelopes corresponding to the randomization order to assign patients to each treatment arm, the study personnel could read the contents and thereby channel certain patients into the desired treatment (eg, if they believed the investigational treatment was effective, they may channel sicker patients into that arm). This would result in imbalance between the two arms of the study (ie, the intervention arm would have sicker patients while the control arm would have healthier people), resulting in the intervention appearing to be less effective than it truly is.
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Blinding
         </strong>
         – Ideally, all relevant groups should be blinded to treatment assignment. This includes study participants, clinicians, data collectors, outcome assessors, and data analysts. Blinding is not always feasible. Some forms of surgery or behavioral modifications, for example, do not lend themselves to blinding of patients and providers. However, outcome assessors and data analyst can usually be blinded regardless of the type of treatment. "Double blinding" generally refers to blinding of the study participants and at least one of the study investigators, although it may not be clear who was blinded when only "double blinding" is reported. For adequate blinding, treatments with a noticeable side effect (eg,
         <a class="drug drug_general" data-topicid="9695" href="/z/d/drug information/9695.html" rel="external">
          niacin
         </a>
         ) ideally should have an "active control" that mimics the side effect.
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Differences between study groups
         </strong>
         – Differences in the treatment groups at baseline can lead to biased results. The goal of randomization is to balance important prognostic variables relevant to the outcome(s) of interest among the different treatment groups. However, randomization is not always successful. Differences in treatment groups typically occur in trials with relatively small numbers of subjects. Researchers can attempt to adjust for baseline differences in the statistical analysis, but it is far more preferable to have balanced groups at baseline.
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Attrition and incomplete reporting
         </strong>
         – High rates of withdrawal of participants from a study may indicate a fundamental problem with the study design. Uneven withdrawal from different study groups can lead to bias, particularly if the reasons for withdrawal differ between, and are related to, the interventions (such as ascribing adverse events to the intervention or lack of effectiveness to the placebo). Reports should describe the reasons for patient withdrawal to allow assessment of their effect on bias and study applicability.
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Early termination for benefit
         </strong>
         – Stopping a trial early for benefit will, on average, overestimate treatment effects [
         <a href="#rid31">
          31
         </a>
         ]. However, the degree of overestimation varies. Small trials that are stopped early with few events can result in large overestimates. In larger trials with more events (ie, &gt;200 to 300 events), early stopping is less likely to result in serious overestimation [
         <a href="#rid32">
          32
         </a>
         ]. Early termination of a trial for harm can also introduce bias (ie, overestimation of the harm); however, it is generally considered ethically obligatory to stop the trial in such circumstances. Early termination for other reasons (eg, slow accrual) is not considered a source of bias per se, though it can sometimes indicate that there are other problems with the trial (eg, the eligibility criteria may be too strict and not reflective of the patient population seen in actual clinical practice).
        </p>
        <p>
        </p>
        <p>
         Other factors that may be considered when assessing the methodologic quality of a study include the accuracy of reporting (eg, details of study methodology, patient characteristics, and study results) and the appropriateness of statistical analyses. For example, an intention to treat (ITT) analysis is appropriate for assessing efficacy of a treatment since it preserves the comparability of treatment groups achieved by randomization. In some cases, it may be appropriate to perform a per protocol analysis alongside the ITT analysis, but when performed alone, per protocol analyses can lead to biased results.
        </p>
        <p>
         The RoB assessment involves judgement. For this reason, it should generally be performed independently by two separate reviewers and there should be a process for resolving disagreements.
        </p>
        <p class="headingAnchor" id="H110188885">
         <span class="h2">
          Meta-analysis
         </span>
        </p>
        <p class="headingAnchor" id="H2326494">
         <span class="h3">
          Statistical methods for combining data
         </span>
         <span class="headingEndMark">
          —
         </span>
         Meta-analysis combines results across studies to provide overall estimates and confidence intervals of treatment effects. For dichotomous outcomes (ie, outcomes with two possible states, such as death versus survival), results are summarized using an odds ratio (OR), relative risk (RR; also called risk ratio), or hazard ratio (HR). Essentially, any study metric can be meta-analyzed, including continuous variables (mean, mean difference, percent change) or proportions. However, meta-analysis is not feasible if the studies measured completely different outcomes (eg, one trial measured pain scores while the other measured functional ability).
        </p>
        <p>
         There are numerous specific methodologic details of meta-analysis that are beyond the scope of this topic. The primary consideration is whether the summary effect estimate should be calculated under the assumption of a "random effects" or a "fixed effect" model [
         <a href="#rid33">
          33
         </a>
         ]. For most of the medical literature, the random effects model is the more appropriate approach. These two approaches are discussed in detail below. (See
         <a class="local">
          'Fixed versus random effects models'
         </a>
         below.)
        </p>
        <p class="headingAnchor" id="H2326501">
         <span class="h3">
          When to combine studies
         </span>
         <span class="headingEndMark">
          —
         </span>
         The decision to combine studies should be based upon both qualitative and quantitative evaluations. Important qualitative features include the degree of similarity of populations, interventions, outcomes, study objectives, and study designs that incorporate both clinical and biologic plausibility. The systematic reviewers should provide a sufficient explanation of the rationale for combining studies to allow the readers to judge for themselves whether they agree that it was appropriate to combine the individual studies.
        </p>
        <p>
         Some investigators also examine statistical heterogeneity (eg, the I
         <sup>
          2
         </sup>
         index or Q statistic) to determine whether it is appropriate to combine data. However, it is not standard practice to avoid meta-analysis solely due to statistical heterogeneity. (See
         <a class="local">
          'Statistical heterogeneity'
         </a>
         below.)
        </p>
        <p class="headingAnchor" id="H3713749953">
         <span class="h3">
          Exploration of heterogeneity
         </span>
         <span class="headingEndMark">
          —
         </span>
         Meta-analyses typically attempt to explore the reasons for statistical heterogeneity across studies (why the results or effect sizes differ from study to study). This is most commonly accomplished by performing subgroup analyses. Meta-regression and sensitivity analyses are also used.
        </p>
        <p>
         These analyses may be exploratory post hoc examinations of differences between studies, or they may be performed to evaluate specific
         <em>
          a priori
         </em>
         hypotheses regarding factors that are thought to impact the effect size (eg, low- versus high-risk patients, earlier versus later treatment).
        </p>
        <p>
         All explorations of heterogeneity carry the risks associated with data dredging and ecological fallacy.
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Data dredging – This refers to analyzing a large number of variables regardless of clinical relevance, which often results in false-positive findings [
         <a href="#rid34">
          34
         </a>
         ]).
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Ecological fallacy – It may be difficult to account properly for certain patient-level variables, such as age, when performing these analyses. For example, most studies report the average for such variables (eg, a mean age of 47 years) which does not reflect the range of values across the study population. Making an assumption about individual data based upon aggregated statistics (known as "ecological fallacy") can produce invalid results in subgroup analyses or meta-regression [
         <a href="#rid35">
          35,36
         </a>
         ]. It is generally not appropriate to make inferences about specific individuals based upon aggregated statistics for groups of individuals. The only reliable way to address this is to analyze patient-level data. (See
         <a class="local">
          'Individual patient data'
         </a>
         below.)
        </p>
        <p>
        </p>
        <p>
         Most findings from subgroup analyses and meta-regression should be considered hypothesis-generating, rather than conclusive.
        </p>
        <p class="headingAnchor" id="H2043857182">
         <span class="h4">
          Subgroup analyses
         </span>
         <span class="headingEndMark">
          —
         </span>
         The primary method used to explore heterogeneity is subgroup analysis, which involves performing separate analyses based upon clinically relevant variables. To minimize the risk of drawing false conclusions, subgroup analyses in meta-analyses should be:
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Specified
         <em>
          a priori
         </em>
         , including hypotheses for the direction of the differences (ie, they should be based upon prior evidence or knowledge)
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Limited to only a few (ie, to avoid data dredging)
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Analyzed by conducting statistical testing for interaction (ie, determining the p value for the between-group difference) rather than simply comparing the separate effect estimates
        </p>
        <p>
        </p>
        <p>
         An example of a subgroup analysis is shown in panel C of the figure (
         <a class="graphic graphic_figure graphicRef121523" href="/z/d/graphic/121523.html" rel="external">
          figure 1
         </a>
         ), which is from a meta-analysis examining the effect of corticosteroids in patients with acute respiratory distress syndrome. The subgroup analysis explored whether the effect differed in studies in which patients received earlier (before day 14) or later (day 14 or later) corticosteroid therapy. In this case, the test for subgroup effect (ie, interaction) was statistically significant (p=0.003).
        </p>
        <p>
         The approach to evaluating findings from subgroup analyses in meta-analyses and clinical trials is discussed in greater detail separately. (See
         <a class="medical medical_review" href="/z/d/html/2763.html" rel="external">
          "Evidence-based medicine", section on 'Subgroup analyses'
         </a>
         .)
        </p>
        <p class="headingAnchor" id="H2327476">
         <span class="h4">
          Meta-regression
         </span>
         <span class="headingEndMark">
          —
         </span>
         Regression analysis of primary studies may be used to account for potential confounding factors and explain differences in results among studies. This meta-analytic technique is commonly known as meta-regression. In this approach, the dependent variable in the regression is the estimate of treatment effect from each individual study and the independent variables are the aggregated characteristics in the individual studies (variables such as drug dose, treatment duration, study size). Instead of individual patients serving as the units of analysis, each individual study is considered to be one observation [
         <a href="#rid37">
          37-39
         </a>
         ]. Meta-regression tests the statistical interaction between the study variable (eg, drug dose) and the treatment effect (eg, relative risk of death). It can include categorical and continuous variables which can be analyzed singly (univariable analysis) or together (multivariable analysis).
        </p>
        <p>
         However, as previously discussed, a common pitfall in meta-regression is to analyze by aggregate data as proxies for patient-level data (termed "ecological fallacy") [
         <a href="#rid35">
          35,36
         </a>
         ]. For example, comparisons between men and women are valid only if all participants in the study are male or female. Meta-regressions that assess for differences in effect size according to sex by including the percent female or male patients within each study assume that each individual in the study is that percentage male or female.
        </p>
        <p>
         Results of meta-regression can be depicted graphically using a bubble plot as shown in the figure (
         <a class="graphic graphic_figure graphicRef69060" href="/z/d/graphic/69060.html" rel="external">
          figure 3
         </a>
         ), which is an example of a meta-regression of early trials of
         <a class="drug drug_general" data-topicid="10053" href="/z/d/drug information/10053.html" rel="external">
          zidovudine
         </a>
         monotherapy for HIV infection [
         <a href="#rid40">
          40
         </a>
         ]. The meta-regression successfully explains the heterogeneity across studies, showing an association between treatment duration and mortality benefit that was not apparent within the individual trials.
        </p>
        <p class="headingAnchor" id="H4220460158">
         <span class="h3">
          Special adaptations of meta-analysis
         </span>
        </p>
        <p class="headingAnchor" id="H2326516">
         <span class="h4">
          Individual patient data
         </span>
         <span class="headingEndMark">
          —
         </span>
         It is sometimes possible to obtain original patient-level data which can be reanalyzed in a meta-analysis [
         <a href="#rid41">
          41
         </a>
         ]. Pooling individual patient data is the most rigorous form of meta-analysis. While more costly and time-consuming and limited by difficulties collecting the original data, there are several benefits, including:
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         It permits regressions of patient-level predictors (eg, age) without the risk of ecological fallacy
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         It allows time-to-event analyses
        </p>
        <p>
        </p>
        <p class="headingAnchor" id="H2326523">
         <span class="h4">
          Network meta-analysis
         </span>
         <span class="headingEndMark">
          —
         </span>
         When multiple different interventions are compared across trials, a network of studies can be established where all the studied interventions are linked to each other by individual trials. Network meta-analysis (NMA) evaluates all studies and all interventions simultaneously to produce multiple pairwise estimates of relative effects of each intervention compared with every other intervention [
         <a href="#rid42">
          42,43
         </a>
         ].
        </p>
        <p>
         A schematic representation of a network diagram is shown in the figure (
         <a class="graphic graphic_figure graphicRef132754" href="/z/d/graphic/132754.html" rel="external">
          figure 4
         </a>
         ). In reality, some network diagrams in NMAs are far more complex (
         <a class="graphic graphic_figure graphicRef132756" href="/z/d/graphic/132756.html" rel="external">
          figure 5
         </a>
         ).
        </p>
        <p>
         The pairwise comparisons in NMAs are based upon both direct and indirect comparisons. For example, consider two drugs (drug A and drug B) that were each evaluated in placebo-controlled trials and directly compared with one another in a separate clinical trial (
         <a class="graphic graphic_figure graphicRef132754" href="/z/d/graphic/132754.html" rel="external">
          figure 4
         </a>
         ). NMA can be used to estimate the relative efficacy of drug A versus drug B based upon the direct comparison (ie, from the trial directly comparing drug A to drug B) and indirect comparisons (ie, from the placebo-controlled trials). The direct and indirect estimates are then pooled together to yield an overall estimate (or "network estimate") of the relative effect. Typically, the direct, indirect, and network estimates are reported separately in NMAs. Some of the comparisons in a NMA may be based entirely on indirect data.
        </p>
        <p>
         When assessing the validity of an NMA, many of the same principles that are used for assessing conventional meta-analysis apply (eg, was the literature search comprehensive, were eligibility criteria for the studies clearly stated, were the individual studies assessed for RoB, how precise are the effect estimates, etc (
         <a class="graphic graphic_table graphicRef56270" href="/z/d/graphic/56270.html" rel="external">
          table 2
         </a>
         )). However, there are two concerns that are unique to NMAs [
         <a href="#rid44">
          44,45
         </a>
         ]:
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Intransitivity
         </strong>
         – The assumption of transitivity is fundamental to NMA because the network estimates rely upon indirect comparisons. For the transitivity assumption to hold, the individual studies must be sufficiently similar in all respects other than the treatments being compared (ie, similar participants, setting, ancillary treatments, and other relevant parameters). In the example above, if studies of drug A versus placebo are systematically different than studies of drug B versus placebo (eg, if they were conducted in an earlier era), then the indirect comparison of drug A versus drug B may be biased due to these differences (ie, the difference may be partly explained by differences in disease management over the intervening decades).
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Incoherence
         </strong>
         – Incoherence (also called inconsistency) refers to differences between the direct and indirect estimates. Incoherence can be a consequence of bias due to methodologic limitations of the studies, publication bias, indirectness, or intransitivity. If the direct and indirect estimates are considerably different from each other, the network estimate may not be valid. Addressing incoherence and assessing its impact on the network estimate requires judgement [
         <a href="#rid44">
          44
         </a>
         ].
        </p>
        <p>
        </p>
        <p>
         Bayesian methods are commonly used to conduct NMA [
         <a href="#rid46">
          46
         </a>
         ]. This approach has the advantage of allowing estimation of the probability of each intervention being best, which, in turn, allows interventions to be ranked. Such ranking, however, needs to be interpreted cautiously, as it can be unstable, depending on the network topology, and can have a substantial degree of imprecision [
         <a href="#rid47">
          47
         </a>
         ].
        </p>
        <p class="headingAnchor" id="H2326530">
         <span class="h1">
          READING AND INTERPRETING A SYSTEMATIC REVIEW
         </span>
         <span class="headingEndMark">
          —
         </span>
         Key questions to consider when reading and interpreting a systematic review are summarized in the table (
         <a class="graphic graphic_table graphicRef56270" href="/z/d/graphic/56270.html" rel="external">
          table 2
         </a>
         ). The reader should appraise the systematic reviews for its quality, potential sources of bias, and extent to which the findings are applicable to their specific question. Systematic reviews and meta-analyses are subject to the same biases observed in all research. In addition, the value of a systematic review's conclusions may be limited by the quality and applicability of the individual studies included in the review.
        </p>
        <p class="headingAnchor" id="H2326537">
         <span class="h1">
          GLOSSARY OF TERMS
         </span>
        </p>
        <p class="headingAnchor" id="H110188417">
         <span class="h2">
          Applicability (also called generalizability or directness)
         </span>
         <span class="headingEndMark">
          —
         </span>
         The relevance of a study (or a group of studies) to a population of interest (or an individual patient). This requires an assessment of how similar the subjects of a study are to the population of interest, the relevance of the studied interventions and outcomes, and other PICO features. (See
         <a class="local">
          'PICO method (PICOD, PICOS, PICOTS, others)'
         </a>
         below.)
        </p>
        <p class="headingAnchor" id="H2327798">
         <span class="h2">
          Ecological fallacy (ecological inference fallacy)
         </span>
         <span class="headingEndMark">
          —
         </span>
         An error in interpreting data where inferences are made about specific individuals based upon aggregated statistics for groups of individuals.
        </p>
        <p class="headingAnchor" id="H110188273">
         <span class="h2">
          Fixed versus random effects models
         </span>
         <span class="headingEndMark">
          —
         </span>
         Most meta-analyses use random effects models; a fixed effect model is appropriate only in select circumstances.
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Fixed effect model
         </strong>
         – The central assumption of a fixed effect model is that there is a single true effect and that all trials provide estimates of this one true effect. Meta-analysis thus provides a pooled estimate of the single true effect. A hypothetical model for a fixed effect model meta-analysis is shown in a figure (
         <a class="graphic graphic_figure graphicRef80325" href="/z/d/graphic/80325.html" rel="external">
          figure 6
         </a>
         ).
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         Since the assumption is that there is a single true effect, the fixed effect model assumes that estimates from each study differ solely because of random error. This assumes that all studies represent the same population, intervention, comparator, and outcome for which there is a single "true" effect size. Fixed effects models yield effect size estimates by assigning a weight to each individual study estimate that reflects the inherent variability in the results measured (ie, the "within-study variance" related to the standard error of the outcome).
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         There are limited instances when it is appropriate to use a fixed effects model for meta-analysis of clinical trials:
        </p>
        <p>
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         If there is extreme confidence that the studies are comparable (ie, characteristics of the enrolled patients, the type of intervention, comparators and outcome measures) such that any difference across studies is just due to random variation. Such an assumption is typically difficult to justify. One example of an appropriate use of the fixed effects model is meta-analysis of repeated, identical, highly controlled trials in a uniform setting, as may be done by pharmaceutical companies during early testing.
        </p>
        <p>
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         The studies are of rare events in which one form of a fixed effects model (the Peto odds ratio) may be less biased than other methods of pooling data [
         <a href="#rid48">
          48
         </a>
         ].
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Random effects model
         </strong>
         – The central assumption of a random effects model is that each study estimate represents a random sample from a distribution of different populations [
         <a href="#rid49">
          49
         </a>
         ]. For most of the medical literature, the random effects model is the more appropriate approach. A hypothetical model for a random effects model meta-analysis is shown in the figure (
         <a class="graphic graphic_figure graphicRef51482" href="/z/d/graphic/51482.html" rel="external">
          figure 7
         </a>
         ). The model assumes there are multiple true treatment effects related to inherent differences in different populations or other factors, and that each trial provides an estimate of its own true effect. The meta-analysis provides a pooled estimate across (or an average of) a range of true effects. Thus, the random effects model assumes that there is not necessarily one "true" effect size but rather that the studies included have provided a glimpse of a range of "true" effects. The random effects model incorporates both "between-study variance" (to capture the range of difference effects across studies) and "within-study variance" (to capture the range of difference effects within studies) [
         <a href="#rid33">
          33
         </a>
         ]. There are several methods for calculating the random effects model estimates. The optimal approaches continue to be debated [
         <a href="#rid50">
          50
         </a>
         ].
        </p>
        <p>
        </p>
        <p class="headingAnchor" id="H110188336">
         <span class="h2">
          Forest plot
         </span>
         <span class="headingEndMark">
          —
         </span>
         A forest plot is a graphical presentation of individual studies, typically displayed as point estimates with their associated 95% CIs on an appropriate scale, next to a description of the individual studies (
         <a class="graphic graphic_figure graphicRef59845" href="/z/d/graphic/59845.html" rel="external">
          figure 8
         </a>
         ). The forest plot allows the reader to see the estimate and the precision of the individual studies, appreciate the heterogeneity of results, and compare the estimates of the individual studies to the overall summary estimate.
        </p>
        <p>
         Ideally, a forest plot should provide sufficient data for the reader to make some assessment of the individual studies in the context of the overall summary (eg, to compare sample sizes, any variations in treatments such as dose, baseline values, demographic features, and study quality).
        </p>
        <p class="headingAnchor" id="H110188387">
         <span class="h2">
          Funnel plot
         </span>
         <span class="headingEndMark">
          —
         </span>
         A graphical technique, with related statistical tests, to examine the studies within a systematic review for the possibility of publication bias (
         <a class="graphic graphic_figure graphicRef65012" href="/z/d/graphic/65012.html" rel="external">
          figure 2
         </a>
         ). (See
         <a class="local">
          'Publication and reporting bias'
         </a>
         above.)
        </p>
        <p class="headingAnchor" id="H110188399">
         <span class="h2">
          Grey literature
         </span>
         <span class="headingEndMark">
          —
         </span>
         A term that is variably defined but generally includes sources of evidence beyond the peer-reviewed published literature. Examples include conference abstracts and proceedings, unpublished study results (eg, available on clinical trial registries such as ), press releases, adverse events databases, other online databases, government agency databases and policy documents (eg, US Food and Drug Administration), unpublished industry data, and dissertations.
        </p>
        <p class="headingAnchor" id="H110188136">
         <span class="h2">
          Heterogeneity
         </span>
        </p>
        <p class="headingAnchor" id="H110188185">
         <span class="h3">
          Clinical heterogeneity
         </span>
         <span class="headingEndMark">
          —
         </span>
         Qualitative differences in study features, such as study eligibility criteria, interventions, or methods of measuring outcomes, that may preclude appropriate meta-analysis. These features can be explicit (such as different drug doses used) or implicit (such as differences in populations depending on setting or country). Clinical heterogeneity may or may not result in statistical heterogeneity but often may not: for example, if the effect size is similar regardless of the drug dose, of the individual drug within a class of drugs, or in different populations (eg, men and women, or Japanese and American).
        </p>
        <p class="headingAnchor" id="H2937551449">
         <span class="h3">
          Statistical heterogeneity
         </span>
         <span class="headingEndMark">
          —
         </span>
         Quantitative differences in study results across studies examining similar questions. Statistical heterogeneity may be due to clinical heterogeneity or to chance. Statistical heterogeneity is measured with a variety of tests, most commonly I
         <sup>
          2
         </sup>
         and the Q statistic. Other heterogeneity measures (eg, H
         <sup>
          2
         </sup>
         , R
         <sup>
          2
         </sup>
         , tau
         <sup>
          2
         </sup>
         ) have also been described but are infrequently used.
        </p>
        <p class="headingAnchor" id="H2043631671">
         <span class="h3">
          I2 index
         </span>
         <span class="headingEndMark">
          —
         </span>
         The I
         <sup>
          2
         </sup>
         index represents the amount of variability in the effect sizes across studies that can be explained by between-study variability. For example, an I
         <sup>
          2
         </sup>
         value of 75 percent means that 75 percent of the variability in the measured effect sizes across studies is caused by true heterogeneity among studies. By consensus, standard thresholds for the interpretation of I
         <sup>
          2
         </sup>
         are 25, 50, and 75 percent to represent low, medium, and high heterogeneity, respectively [
         <a href="#rid51">
          51
         </a>
         ]. However, the investigators who introduced the I
         <sup>
          2
         </sup>
         statistic noted that naïve categorization of I
         <sup>
          2
         </sup>
         values is not appropriate in all circumstances and that "the practical impact of heterogeneity in a meta-analysis also depends on the size and direction of treatment effects" [
         <a href="#rid51">
          51
         </a>
         ]. The clinical implication and interpretability of a meta-analysis with a large I
         <sup>
          2
         </sup>
         index will be different for studies with large statistically significant effects compared with studies with smaller inconsistent effects.
        </p>
        <p class="headingAnchor" id="H2307188578">
         <span class="h3">
          Q statistic
         </span>
         <span class="headingEndMark">
          —
         </span>
         The Q statistic (or chi square test for heterogeneity) tests the hypothesis that results across studies are homogeneous. Its calculation involves summing the squared deviations from the effect measured in each study from the overall effect and weighting the contribution from each study by the inverse of its variance. The Q statistic is usually interpreted to indicate heterogeneity if its P value is &lt;0.10. A nonsignificant value suggests that the studies are homogeneous. However, the Q statistic has limited power to detect heterogeneity in meta-analyses with few studies, while it tends to over-detect heterogeneity in meta-analyses with many studies [
         <a href="#rid52">
          52
         </a>
         ].
        </p>
        <p class="headingAnchor" id="H110188324">
         <span class="h2">
          Meta-regression
         </span>
         <span class="headingEndMark">
          —
         </span>
         A meta-analytic technique that permits adjustment for potential confounders and analysis of different variables to help explain differences in results across studies. Equivalent to patient-level regression, except that the unit of analysis is a study instead of a person. Additional details are provided above. (See
         <a class="local">
          'Meta-regression'
         </a>
         above.)
        </p>
        <p class="headingAnchor" id="H110188312">
         <span class="h2">
          Network meta-analysis
         </span>
         <span class="headingEndMark">
          —
         </span>
         A technique to simultaneously meta-analyze a network of studies that evaluated related, but different, specific comparisons. It permits quantitative inferences across studies that have made indirect comparisons of interventions. An example would be the comparison of two or more drugs to each other, when each was studied only in comparison to placebo. Additional details are provided above. (See
         <a class="local">
          'Network meta-analysis'
         </a>
         above.)
        </p>
        <p class="headingAnchor" id="H110188348">
         <span class="h2">
          PICO method (PICOD, PICOS, PICOTS, others)
         </span>
         <span class="headingEndMark">
          —
         </span>
         An acronym that stands for
         <strong>
          P
         </strong>
         opulation,
         <strong>
          I
         </strong>
         ntervention(s),
         <strong>
          C
         </strong>
         omparator(s),
         <strong>
          O
         </strong>
         utcome(s); added letters include Study
         <strong>
          D
         </strong>
         esign (PICOD),
         <strong>
          S
         </strong>
         etting (PICOS),
         <strong>
          T
         </strong>
         iming and Setting (PICOTS). Some factors may be used instead (eg,
         <strong>
          E
         </strong>
         xposure instead of
         <strong>
          I
         </strong>
         ntervention) and other factors may also be import (eg, effect modifiers). PICO is the basis for a systematic approach in developing a research question and research protocol. While used extensively for systematic reviews, PICO is relevant to all medical research questions. Each feature is defined explicitly and comprehensively so that it is unambiguously evident which studies are eligible for inclusion in a systematic review. (See
         <a class="medical medical_review" href="/z/d/html/2763.html" rel="external">
          "Evidence-based medicine", section on 'Formulating a clinical question'
         </a>
         .)
        </p>
        <p class="headingAnchor" id="H110188452">
         <span class="h2">
          PRISMA
         </span>
         <span class="headingEndMark">
          —
         </span>
         The Preferred Reporting Items of Systematic reviews and Meta-Analyses (PRISMA)
         <a class="external" href="/external-redirect?target_url=http%3A%2F%2Fwww.prisma-statement.org%2FPRISMAStatement%2F&amp;token=mGCp3jf%2BU%2BRF8NnZx3snHv23JvyZHFuw2BQvvMqH%2FTjuSTaZubvmEN7d8eT5LCAuVHgPyeHoFebXdZI4k7aHSw%3D%3D&amp;TOPIC_ID=16293" target="_blank">
          statement
         </a>
         and
         <a class="external" href="/external-redirect?target_url=http%3A%2F%2Fwww.prisma-statement.org%2FExtensions%2FDefault.aspx&amp;token=i20xYI8aSBFiAGc5wtYRs651Bna3iA4Qk4trdocUrr%2B98tPGhICW5bd%2Bx4knFhqLMwhc4JBEU3LrKG2dfTXb4g%3D%3D&amp;TOPIC_ID=16293" target="_blank">
          extensions
         </a>
         are sets of guidelines for reporting systematic reviews and meta-analyses [
         <a href="#rid1">
          1
         </a>
         ]. PRISMA is used as a standard by many researchers and journals.
        </p>
        <p class="headingAnchor" id="H587111879">
         <span class="h2">
          PROSPERO
         </span>
         <span class="headingEndMark">
          —
         </span>
         An international database of prospectively registered systematic reviews in health care.
         <a class="external" href="/external-redirect?target_url=https%3A%2F%2Fwww.crd.york.ac.uk%2FPROSPERO%2F&amp;token=F2fTJXWTbAdqtiUVhUGfESjJfVCAANq24enn%2BCUtAXwU9xYhUYnSrzTBPuDHiQ85&amp;TOPIC_ID=16293" target="_blank">
          PROSPERO
         </a>
         creates a permanent record of systematic review protocols to reduce unnecessary duplication of efforts and increase transparency. Researchers should ideally enter their protocols prospectively and update them as necessary.
        </p>
        <p class="headingAnchor" id="H110188375">
         <span class="h2">
          Publication bias
         </span>
         <span class="headingEndMark">
          —
         </span>
         One of several related biases in the available evidence being considered for inclusion in a systematic review. Conceptually, studies that have been published are systematically different than studies that have failed to be published, due to lack of acceptance by journals, lack of interest by authors or research grantors, or potentially, by deliberate withholding by funders. Theoretically, "positive" (statistically significant) results are more likely to be published than "negative" results.
        </p>
        <p>
         Related biases include:
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Selective outcome reporting bias, wherein published studies report only certain outcomes
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Time-lag bias, wherein "negative" study results tend to be delayed in their publication compared with "positive" results
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Location bias, wherein "positive" or more interesting results tend to be published in journals that are more easily accessible
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Language bias, wherein results of studies published in non-English language journals differ from those of studies from the same countries or authors published in English
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         Multiple or duplicate publication bias, wherein certain studies may be overrepresented in the literature due to duplicate or overlapping publications (which may be difficult to tease apart).
        </p>
        <p>
        </p>
        <p class="headingAnchor" id="H2691248035">
         <span class="h2">
          Risk of bias assessment
         </span>
         <span class="headingEndMark">
          —
         </span>
         The risk of bias (RoB) assessment (sometimes referred to as "quality assessment") represents the extent to which trial design and methodology prevented systematic error. Assessing RoB can help explain differences in the results of systematic reviews. The primary value of the RoB assessment of individual studies in the meta-analysis is to determine the degree of confidence that the pooled effect estimate reflects the "truth" as best as it can be measured. One would be more likely to have high confidence in conclusions based upon "high-quality" (ie, low RoB) studies rather than "low-quality" (ie, high RoB) studies. Additional details are provided above. (See
         <a class="local">
          'Risk of bias assessment'
         </a>
         above.)
        </p>
        <p class="headingAnchor" id="H2327805">
         <span class="h2">
          Sensitivity analysis
         </span>
         <span class="headingEndMark">
          —
         </span>
         A method of exploring heterogeneity
         <strong>
         </strong>
         in a meta-analysis by varying which studies are included to determine the effects of such changes. Used to explore how sensitive a meta-analysis finding is to inclusion of individual studies and to evaluate possible causes of heterogeneity; for example, whether exclusion of high RoB studies influences the size of the effect.
        </p>
        <p class="headingAnchor" id="H110188989">
         <span class="h1">
          SUMMARY
         </span>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Definitions
         </strong>
        </p>
        <p>
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         A
         <strong>
          systematic review
         </strong>
         is a comprehensive summary of all available evidence that meets predefined eligibility criteria to address a specific clinical question or range of questions. (See
         <a class="local">
          'Systematic review'
         </a>
         above.)
        </p>
        <p>
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         <strong>
          Meta-analysis
         </strong>
         , which is commonly included in systematic reviews, is a statistical method that quantitatively combines the results from different studies. It is commonly used to provide an overall pooled estimate of the benefit or harm of an intervention. (See
         <a class="local">
          'Meta-analysis'
         </a>
         above.)
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Steps to conducting systematic review and meta-analysis
         </strong>
         – Several steps are essential for conducting a systematic review or meta-analysis. These include:
        </p>
        <p>
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         Formulating the research question(s) (see
         <a class="local">
          'Formulating research questions'
         </a>
         above)
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         Developing a protocol (see
         <a class="local">
          'Developing a protocol'
         </a>
         above)
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         Searching for the evidence (see
         <a class="local">
          'The literature search'
         </a>
         above)
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         Assessing the risk of bias of studies (see
         <a class="local">
          'Risk of bias assessment'
         </a>
         above)
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         Summarizing and displaying results (eg, using forest pots and a summary of findings table, as shown in the figure (
         <a class="graphic graphic_figure graphicRef121523" href="/z/d/graphic/121523.html" rel="external">
          figure 1
         </a>
         )) (see
         <a class="local">
          'Forest plot'
         </a>
         above)
        </p>
        <p class="bulletIndent2">
         <span class="glyph">
          •
         </span>
         Exploring reasons for heterogeneity across studies (see
         <a class="local">
          'Exploration of heterogeneity'
         </a>
         above)
        </p>
        <p>
        </p>
        <p class="bulletIndent1">
         <span class="glyph">
          ●
         </span>
         <strong>
          Reading and interpreting systematic reviews
         </strong>
         – When reading and interpreting a systematic review, the reader should appraise the methodologic quality, assess for potential sources of bias, and consider the extent to which the findings are applicable to their specific question. Key issues to consider are summarized in the table (
         <a class="graphic graphic_table graphicRef56270" href="/z/d/graphic/56270.html" rel="external">
          table 2
         </a>
         ). The value of a systematic review's conclusions may be limited by the quality and applicability of the individual studies included in the review. (See
         <a class="local">
          'Reading and interpreting a systematic review'
         </a>
         above.)
        </p>
        <p>
        </p>
       </div>
       <div class="headingAnchor" id="references">
        <ol id="reference">
         <li>
          <a class="nounderline abstract_t">
           Page MJ, McKenzie JE, Bossuyt PM, et al. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ 2021; 372:n71.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Page MJ, Moher D, Bossuyt PM, et al. PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews. BMJ 2021; 372:n160.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Ioannidis JP. Why most published research findings are false: author's reply to Goodman and Greenland. PLoS Med 2007; 4:e215.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           LeLorier J, Grégoire G, Benhaddad A, et al. Discrepancies between meta-analyses and subsequent large randomized, controlled trials. N Engl J Med 1997; 337:536.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Cappelleri JC, Ioannidis JP, Schmid CH, et al. Large trials vs meta-analysis of smaller trials: how do their results compare? JAMA 1996; 276:1332.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Villar J, Carroli G, Belizán JM. Predictive ability of meta-analyses of randomised controlled trials. Lancet 1995; 345:772.
          </a>
         </li>
         <li class="breakAll">
          Extensions of the PRISMA Statement. Available at: www.prisma-statement.org/Extensions/Default.aspx (Accessed on August 30, 2023).
         </li>
         <li class="breakAll">
          Institute of Medicine. Finding what workds in health care: Standards for systematic reviews. The National Academies Press, Washington, DC, 2011. Available at: http://www.iom.edu/Reports/2011/Finding-What-Works-in-Health-Care-Standards-for-Systematic-Reviews.aspx (Accessed on October 10, 2011).
         </li>
         <li class="breakAll">
          Cochrane. Available at: https://www.cochrane.org/ (Accessed on June 19, 2023).
         </li>
         <li>
          <a class="nounderline abstract_t">
           Counsell C. Formulating questions and locating primary studies for inclusion in systematic reviews. Ann Intern Med 1997; 127:380.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Paez A. Grey literature: An important resource in systematic reviews. J Evid Based Med 2017.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Schmucker CM, Blümle A, Schell LK, et al. Systematic review finds that study data not published in full text articles have unclear impact on meta-analyses results in medical research. PLoS One 2017; 12:e0176210.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Thornton A, Lee P. Publication bias in meta-analysis: its causes and consequences. J Clin Epidemiol 2000; 53:207.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Ioannidis JP. Effect of the statistical significance of results on the time to completion and publication of randomized efficacy trials. JAMA 1998; 279:281.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Vevea JL, Woods CM. Publication bias in research synthesis: sensitivity analysis using a priori weight functions. Psychol Methods 2005; 10:428.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Egger M, Davey Smith G, Schneider M, Minder C. Bias in meta-analysis detected by a simple, graphical test. BMJ 1997; 315:629.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Tang JL, Liu JL. Misleading funnel plot for detection of bias in meta-analysis. J Clin Epidemiol 2000; 53:477.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Terrin N, Schmid CH, Lau J, Olkin I. Adjusting for publication bias in the presence of heterogeneity. Stat Med 2003; 22:2113.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Terrin N, Schmid CH, Lau J. In an empirical evaluation of the funnel plot, researchers could not visually identify publication bias. J Clin Epidemiol 2005; 58:894.
          </a>
         </li>
         <li class="breakAll">
          Methods Guide for Effectiveness and Comparative Effectiveness Reviews, Agency for Healthcare Research and Quality (US).
         </li>
         <li>
          <a class="nounderline abstract_t">
           Duval S, Tweedie R. Trim and fill: A simple funnel-plot-based method of testing and adjusting for publication bias in meta-analysis. Biometrics 2000; 56:455.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Copas J. What works?: Selectivity models and meta-analysis. Journal of the Royal Statistical Society Series A 1999; 162:95.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Rosenthal R. The 'file drawer problem' and tolerance for null results. Psychol Bull 1979; 86:638.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Ioannidis JP, Trikalinos TA. An exploratory test for an excess of significant findings. Clin Trials 2007; 4:245.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Moher D, Jadad AR, Nichol G, et al. Assessing the quality of randomized controlled trials: an annotated bibliography of scales and checklists. Control Clin Trials 1995; 16:62.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Higgins JP, Altman DG, Gøtzsche PC, et al. The Cochrane Collaboration's tool for assessing risk of bias in randomised trials. BMJ 2011; 343:d5928.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Sterne JAC, Savović J, Page MJ, et al. RoB 2: a revised tool for assessing risk of bias in randomised trials. BMJ 2019; 366:l4898.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Sterne JA, Hernán MA, Reeves BC, et al. ROBINS-I: a tool for assessing risk of bias in non-randomised studies of interventions. BMJ 2016; 355:i4919.
          </a>
         </li>
         <li class="breakAll">
          Campbell JM, Klugar M, Ding S, et al. Diagnostic test accuracy systematic reviews. In: JBI Manual for Evidence Synthesis, Aromataris E, Munn Z (Eds), JBI, 2020.
         </li>
         <li>
          <a class="nounderline abstract_t">
           Verhagen AP, de Vet HC, de Bie RA, et al. The art of quality assessment of RCTs included in systematic reviews. J Clin Epidemiol 2001; 54:651.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Bassler D, Briel M, Montori VM, et al. Stopping randomized trials early for benefit and estimation of treatment effects: systematic review and meta-regression analysis. JAMA 2010; 303:1180.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Walter SD, Guyatt GH, Bassler D, et al. Randomised trials with provision for early stopping for benefit (or harm): The impact on the estimated treatment effect. Stat Med 2019; 38:2524.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           DerSimonian R, Laird N. Meta-analysis in clinical trials. Control Clin Trials 1986; 7:177.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Schulz KF, Grimes DA. Multiplicity in randomised trials II: subgroup and interim analyses. Lancet 2005; 365:1657.
          </a>
         </li>
         <li class="breakAll">
          Rothman KJ, Greenland S. Modern epidemiology, 2nd ed, Lippincott-Raven, Philadelphia 1998.
         </li>
         <li>
          <a class="nounderline abstract_t">
           Geissbühler M, Hincapié CA, Aghlmandi S, et al. Most published meta-regression analyses based on aggregate data suffer from methodological pitfalls: a meta-epidemiological study. BMC Med Res Methodol 2021; 21:123.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Berkey CS, Hoaglin DC, Mosteller F, Colditz GA. A random-effects regression model for meta-analysis. Stat Med 1995; 14:395.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Schmid CH. Exploring heterogeneity in randomized trials via metaanalysis. Drug Inf J 1999; 33:211.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           van Houwelingen HC, Arends LR, Stijnen T. Advanced methods in meta-analysis: multivariate approach and meta-regression. Stat Med 2002; 21:589.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Ioannidis JP, Cappelleri JC, Sacks HS, Lau J. The relationship between study design, results, and reporting of randomized clinical trials of HIV infection. Control Clin Trials 1997; 18:431.
          </a>
         </li>
         <li class="breakAll">
          Clarke MJ, Stewart LA. Principles of and procedures for systematic reviews. In: Systematic reviews in health care: meta-analysis in context, Egger M, Smith G, Altman D (Eds), BMJ Publishing Group, London 2001. p.23.
         </li>
         <li>
          <a class="nounderline abstract_t">
           Jansen JP, Fleurence R, Devine B, et al. Interpreting indirect treatment comparisons and network meta-analysis for health-care decision making: report of the ISPOR Task Force on Indirect Treatment Comparisons Good Research Practices: part 1. Value Health 2011; 14:417.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Mills EJ, Ioannidis JP, Thorlund K, et al. How to use an article reporting a multiple treatment comparison meta-analysis. JAMA 2012; 308:1246.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Brignardello-Petersen R, Mustafa RA, Siemieniuk RAC, et al. GRADE approach to rate the certainty from a network meta-analysis: addressing incoherence. J Clin Epidemiol 2019; 108:77.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Brignardello-Petersen R, Bonner A, Alexander PE, et al. Advances in the GRADE approach to rate the certainty in estimates from a network meta-analysis. J Clin Epidemiol 2018; 93:36.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Salanti G. Indirect and mixed-treatment comparison, network, or multiple-treatments meta-analysis: many names, many benefits, many concerns for the next generation evidence synthesis tool. Res Synth Methods 2012; 3:80.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Trinquart L, Attiche N, Bafeta A, et al. Uncertainty in Treatment Rankings: Reanalysis of Network Meta-analyses of Randomized Trials. Ann Intern Med 2016; 164:666.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Bradburn MJ, Deeks JJ, Berlin JA, Russell Localio A. Much ado about nothing: a comparison of the performance of meta-analytical methods with rare events. Stat Med 2007; 26:53.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Lau J, Ioannidis JP, Schmid CH. Summing up evidence: one answer is not always enough. Lancet 1998; 351:123.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Jackson D, Law M, Stijnen T, et al. A comparison of seven random-effects models for meta-analyses that estimate the summary odds ratio. Stat Med 2018; 37:1059.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Higgins JP, Thompson SG, Deeks JJ, Altman DG. Measuring inconsistency in meta-analyses. BMJ 2003; 327:557.
          </a>
         </li>
         <li>
          <a class="nounderline abstract_t">
           Huedo-Medina TB, Sánchez-Meca J, Marín-Martínez F, Botella J. Assessing heterogeneity in meta-analysis: Q statistic or I2 index? Psychol Methods 2006; 11:193.
          </a>
         </li>
        </ol>
       </div>
       <div id="topicVersionRevision">
        Topic 16293 Version 27.0
       </div>
      </div>
      <div class="row">
       <div class="col-12">
        <h4 style="text-align: center;padding: 20px;color: cadetblue;">
         References
        </h4>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/33782057" id="rid0" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          1 : The PRISMA 2020 statement: an updated guideline for reporting systematic reviews.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/33781993" id="rid1" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          2 : PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/17593900" id="rid2" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          3 : Why most published research findings are false: author's reply to Goodman and Greenland.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/9262498" id="rid3" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          4 : Discrepancies between meta-analyses and subsequent large randomized, controlled trials.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/8861993" id="rid4" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          5 : Large trials vs meta-analysis of smaller trials: how do their results compare?
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/7891492" id="rid5" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          6 : Predictive ability of meta-analyses of randomised controlled trials.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/7891492" id="rid6" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          7 : Predictive ability of meta-analyses of randomised controlled trials.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/7891492" id="rid7" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          8 : Predictive ability of meta-analyses of randomised controlled trials.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/7891492" id="rid8" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          9 : Predictive ability of meta-analyses of randomised controlled trials.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/9273830" id="rid9" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          10 : Formulating questions and locating primary studies for inclusion in systematic reviews.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/29266844" id="rid10" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          11 : Grey literature: An important resource in systematic reviews.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/28441452" id="rid11" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          12 : Systematic review finds that study data not published in full text articles have unclear impact on meta-analyses results in medical research.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/10729693" id="rid12" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          13 : Publication bias in meta-analysis: its causes and consequences.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/9450711" id="rid13" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          14 : Effect of the statistical significance of results on the time to completion and publication of randomized efficacy trials.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/16392998" id="rid14" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          15 : Publication bias in research synthesis: sensitivity analysis using a priori weight functions.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/9310563" id="rid15" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          16 : Bias in meta-analysis detected by a simple, graphical test.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/10812319" id="rid16" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          17 : Misleading funnel plot for detection of bias in meta-analysis.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/12820277" id="rid17" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          18 : Adjusting for publication bias in the presence of heterogeneity.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/16085192" id="rid18" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          19 : In an empirical evaluation of the funnel plot, researchers could not visually identify publication bias.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/16085192" id="rid19" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          20 : In an empirical evaluation of the funnel plot, researchers could not visually identify publication bias.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/10877304" id="rid20" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          21 : Trim and fill: A simple funnel-plot-based method of testing and adjusting for publication bias in meta-analysis.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/" id="rid21" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          22 : What works?: Selectivity models and meta-analysis
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/" id="rid22" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          23 : The 'file drawer problem' and tolerance for null results
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/17715249" id="rid23" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          24 : An exploratory test for an excess of significant findings.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/7743790" id="rid24" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          25 : Assessing the quality of randomized controlled trials: an annotated bibliography of scales and checklists.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/22008217" id="rid25" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          26 : The Cochrane Collaboration's tool for assessing risk of bias in randomised trials.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/31462531" id="rid26" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          27 : RoB 2: a revised tool for assessing risk of bias in randomised trials.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/27733354" id="rid27" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          28 : ROBINS-I: a tool for assessing risk of bias in non-randomised studies of interventions.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/27733354" id="rid28" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          29 : ROBINS-I: a tool for assessing risk of bias in non-randomised studies of interventions.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/11438404" id="rid29" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          30 : The art of quality assessment of RCTs included in systematic reviews.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/20332404" id="rid30" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          31 : Stopping randomized trials early for benefit and estimation of treatment effects: systematic review and meta-regression analysis.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/30887553" id="rid31" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          32 : Randomised trials with provision for early stopping for benefit (or harm): The impact on the estimated treatment effect.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/3802833" id="rid32" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          33 : Meta-analysis in clinical trials.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/15885299" id="rid33" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          34 : Multiplicity in randomised trials II: subgroup and interim analyses.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/15885299" id="rid34" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          35 : Multiplicity in randomised trials II: subgroup and interim analyses.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/34130658" id="rid35" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          36 : Most published meta-regression analyses based on aggregate data suffer from methodological pitfalls: a meta-epidemiological study.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/7746979" id="rid36" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          37 : A random-effects regression model for meta-analysis.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/" id="rid37" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          38 : Exploring heterogeneity in randomized trials via metaanalysis
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/11836738" id="rid38" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          39 : Advanced methods in meta-analysis: multivariate approach and meta-regression.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/9315426" id="rid39" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          40 : The relationship between study design, results, and reporting of randomized clinical trials of HIV infection.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/9315426" id="rid40" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          41 : The relationship between study design, results, and reporting of randomized clinical trials of HIV infection.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/21669366" id="rid41" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          42 : Interpreting indirect treatment comparisons and network meta-analysis for health-care decision making: report of the ISPOR Task Force on Indirect Treatment Comparisons Good Research Practices: part 1.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/23011714" id="rid42" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          43 : How to use an article reporting a multiple treatment comparison meta-analysis.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/30529648" id="rid43" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          44 : GRADE approach to rate the certainty from a network meta-analysis: addressing incoherence.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/29051107" id="rid44" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          45 : Advances in the GRADE approach to rate the certainty in estimates from a network meta-analysis.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/26062083" id="rid45" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          46 : Indirect and mixed-treatment comparison, network, or multiple-treatments meta-analysis: many names, many benefits, many concerns for the next generation evidence synthesis tool.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/27089537" id="rid46" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          47 : Uncertainty in Treatment Rankings: Reanalysis of Network Meta-analyses of Randomized Trials.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/16596572" id="rid47" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          48 : Much ado about nothing: a comparison of the performance of meta-analytical methods with rare events.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/9439507" id="rid48" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          49 : Summing up evidence: one answer is not always enough.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/29315733" id="rid49" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          50 : A comparison of seven random-effects models for meta-analyses that estimate the summary odds ratio.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/12958120" id="rid50" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          51 : Measuring inconsistency in meta-analyses.
         </p>
        </a>
       </div>
       <div class="col-12" style="text-align: left;direction: ltr">
        <a href="https://pubmed.ncbi.nlm.nih.gov/16784338" id="rid51" target="_blank">
         <p class="btn-info round" style='width: 100%;margin-bottom: 5px !important;padding: 10px;border-radius: 12px;;font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;'>
          52 : Assessing heterogeneity in meta-analysis: Q statistic or I2 index?
         </p>
        </a>
       </div>
      </div>
     </div>
    </div>
   </section>
  </section>
  <!-- End Main Menu Area -->
  <!-- Start Search Popup Area -->
  <!-- End Footer Area -->
  <!-- Back to top -->
  <a class="scrolltop" href="#top">
   <i class="icofont-hand-drawn-up">
   </i>
  </a>
  <!-- End Back to top -->
  <!-- jQuery Min JS -->
  <!-- Prpper JS -->
  <!-- Bootstrap Min JS -->
  <!-- Classy Nav Min Js -->
  <!-- Owl Carousel Min Js -->
  <!-- Magnific Popup JS -->
  <!-- CounterUp JS -->
  <!-- Waypoints JS -->
  <!-- Form Validator Min JS -->
  <!-- Contact Form Min JS -->
  <!-- Main JS -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
 </body>
</html>
